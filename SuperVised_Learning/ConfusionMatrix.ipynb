{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9301a0a",
   "metadata": {},
   "source": [
    "#### Evaluation of model  -> confusion matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b80864",
   "metadata": {},
   "source": [
    "1. i have actual data and i have a predicted data \n",
    "2. predicted - 1    0\n",
    "3. 1           TP   FN\n",
    "4. 0           FP   TN           tp->true positive, fn->false negative , fp->false positive , fp->false positive \n",
    "\n",
    "FN , FP -> these 2 are wrong prediction \n",
    "\n",
    "5. tOtal 250 \n",
    "5. predicted   1    0\n",
    "6. 1          190   10\n",
    "7. 0           10   40\n",
    "\n",
    "\n",
    "8. accuracy , precision , recall , f1 Score => these thing we can find \n",
    "\n",
    "1. accuracy -> percentage of correct prediction => formula is TP+TN/(TP+FP+TN+FN); => 230/250 => 0.92 \n",
    "\n",
    "2. precision -> out of all predict positive how many were actually -> TP/(TP+FP); => 190/200 \n",
    "                use when false positive are costly (Ex -> spam detection)\n",
    "\n",
    "3. Recall -> out of all actual positive how many did we correctly predicted \n",
    "                => Tp / TP + FN \n",
    "                use when false negative are costly(disease detection)\n",
    "\n",
    "4. F1 Score -> Harmonic mean of precision and recall \n",
    "                F1-score = 2 * precision * Recall / (precision + Recall);\n",
    "\n",
    "                use when you want balance between precision and recall\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
